== Yoga video processing ==


This is an AI task pipeline to process the input video files that are Yoga training classes. Each task can execute independently if used separately. 
PC reads video files from a smartphone to a video server. Server iterates the given input directory tree to identify the uploaded video files, then compress the video, transcribe to srt, refine the srt, generate metadata for the video. 

Tasks in the pipeline:
1. move-mtp-mp4-windows.ps1: 
	- find and move video files from USB devices to server. Run on PC when USB is plugged.
2. compress_transcribe_metadata.py
	- the entry driver for all tasks: compress the video, transcribe to srt, refine the srt, generate metadata.
3. video_compress.py
	- compress the video with H.265.
4. video_transcribe.py 
	- transcribe to subtitle srt file with local AI models.
	- (with built-in short initial prompt)
5. srt_refine.py 
	- refine the srt file subtitles by correcting the wordings with remote AI models.
	- (prompt file: process_srt.prompt)
6. video_metadata.py 
	- summarize the srt file with remote AI models, generate metadata and embed into the video files.
	- (prompt files:  process_chunk.prompt, process_chapters.prompt, process_summary.prompt)
7. video_transcribe_openai_api.py
	- The same functionality as video_transcribe.py, but use remote API.

-- compress_transcribe_metadata --
Input arguments:
	Input root directory: for original video files,
	Temp root directory: for intermediate files, such as the compressed video files, extracted audio files, generated srt files, metadata files, etc. The temp root dir is used as the output dir of video_compress.py, and as the temp dir of video_transcribe.py, as the working dir of srt_refine.py and video_metadata.py.
	Output root directory: for final results, including the compressed video file with embedded metadata, generated srt file, also the summary and metadata files just for fun.
	The directory tree structures of the three dirs are the same.

	--skip-compression, the video files are compressed already. Skip the compression step. Use the input root directory as the temp root dir for the compressed videos to transcribe and embed metadata. If the output dir is the same as the input dir, no need to move them to the output root dir.
	--erase-original is to remove the original video. default is false.
	--erase-temp is to remove the intermediate files. default is false.
	--model is to specify faster-whisper or openai-whisper for ASR. default is faster-whisper.
	--api is to specify openai or deepseek for summarization. default is deepseek.
	--split to choose how to split the subtitle into chunks, in time or token. default is time.
	--stride to specify the STRIDE value of a time-split chunk (only valid when split is time). default is 15.
	--overlap to specify the OVERLAP value between two chunks (only valid when split is time). default is 5.


-- video-compress --
Compress a video file with H.265 encoder. 
Input arguments: 
    	The path of input video file.
    	The output directory for the result compressed file.
    	--erase-original to indicate if the original video file is removed once compression is done.

Functionality:    
	If the input video file is not H.265 encoded, then compress it with H.265 encoder. Use hardware acceleration hevc_nvenc on host xf-workstation; or software encoder libx265 on host XF-NAS and other hosts. The resulted file has "x265" inserted in the original filename right before the suffix. Conduct the compression in the output directory. Remove the original video files if the compression is finished successfully, and the option --erase-original is optional.

-- video-transcribe --
Transcribe a video file to srt subtitle file. 
Input arguments:
    	The path of the input video file.
	A temp directory for intermediate files like the extracted audio or srt file(s)
    	--model to choose either faster-whisper or openai-whisper for ASR, default is faster-whisper.

Functionality:
	Extract the audio of the input video into the temporary dir, and then use faster-whisper or openai-whisper package to transcribe the audio into subtitle srt file. Both original and target languages are Chinese. Most of the audio is yoga exercise instructions given by the Yoga teacher. An initial prompt is given to the model to assist the transcription. Keep the subtitle file basename same as the video file basename. The temp files are removed once everything is done. 

-- srt-refine --
Refine the srt subtitles by correcting the wordings.
Input arguments:
	The srt file.
    	--api to choose either deepseek or openai.
    	--split to choose how to split the subtitle into chunks, in time duratipon or in token size, ensuring chunk size is within api rate limit.
    	--stride to specify the STRIDE value of a time-split chunk 

Functinality:
	Correct the subtitles with AI model. ASR in video transcription may generate lots of minor errors. A reasonable prompt can help a lot, especially for those Yoga gestures.

-- generate-metadata --
Summarize the yoga training class content into video file metadata.
Input arguments:
	A subtitle file.
	The video of the subtitle.
    	--api to choose either deepseek or openai.
    	--split to choose how to split the subtitle into chunks, in time duratipon or in token size, ensuring chunk size is within api rate limit.
    	--stride to specify the STRIDE value of a time-split chunk 
    	--overlap to specify the OVERLAP value of overlapping duration between two chunks 

Functionality:                                            
	Partition the whole yago class into chunks according to option split, and summarize every chunk with a summary and chapters (each chapter has a start time and main postures). Then summarize the whole-class description based on the chunk summaries. The chapters of different chunks may overlap, so the script merge the overlapped chapers. The summarization is done by calling with deepseek API or OpenAI API. The API keys are given in environment like DEEPSEEK_API_KEY and OPENAI_API_KEY. Then the description and chapters are converted to video metadata file and written into the video file, together with other metadata like artist, creation time, title, etc.

If split is by token, but the API token context limit is large enough for the srt file, then still use time based split, because smaller chunk size makes the posture recognition more accurate. Otherwise, split by token. 
No matter split by time or token, we need some overlap (5min) between two neighbor chunks, avoiding a posture crossing the chunk boundary.

chunk len = stride + overlap 
  stide overlap 
0 --------
        1 --------
                2 --------
                        3 ---------
                          remaining 
           (chunk_len * 0.5 + overlap) <= remaining <= (chunk_len * 1.5)


